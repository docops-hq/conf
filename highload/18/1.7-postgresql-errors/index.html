<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Топ ошибок со стороны разработки при работе с PostgreSQL &middot; Конспекты</title><link rel=stylesheet href=/conf/css/style.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700"><link rel=stylesheet href=/conf/custom.css><link rel=icon type=image/png sizes=32x32 href=/conf/images/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/conf/images/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/conf/images/apple-touch-icon.png><link href rel=alternate type=application/rss+xml title=Конспекты><script async src="https://www.googletagmanager.com/gtag/js?id=UA-107215405-3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-107215405-3');</script></head><body><nav class=nav><div class=nav-container><a href=/conf/><h2 class=nav-title>Конспекты</h2></a><ul><li><a href=/conf/>Posts</a></li></ul></div></nav><main><div class=post><div class=single__header><div class=post-info>Алексей Лесовский, Data Egret</div><h1 class=post-title>Топ ошибок со стороны разработки при работе с PostgreSQL</h1><div class=post-line></div><div class=tag-block><span class=catalogue-tags><a href=/conf/tags/highload class=tags>Highload</a></span><span class=catalogue-tags><a href=/conf/tags/2018 class=tags>2018</a></span></div></div><h1 id=intro>Intro</h1><p>В компаниях любого размера бывают проблемы. Откуда они берутся?</p><ol><li><p>Из фич. Начинаем использовать продвинутые фичи, утилиты и прочее. «Хочется взять дежурный пистолет, положить в ящик стола, иногда достать, застрелиться и работать дальше».</p></li><li><p>Из хранения данных. Когда оно усложняется, больше шансов написать кривой запрос.</p></li><li><p>Из жизненного цикла. Разработчики пилят, админы настраивают, а улучшать систему некому. База работает с дефолтными конфигами и когда-нибудь ломается.</p></li></ol><h1 id=наводим-порядок>Наводим порядок</h1><h2 id=планирование-и-мониторинг>Планирование и мониторинг</h2><p>Пока проект новый, всё работает быстро. Потом приложение растёт, появляется больше клиентов — приложение тормозит. Где могут быть проблемы:</p><ul><li>Диск занят <code>pg_xlog</code>. Разработчик: «а давайте его удалим!». Не надо так. Там журнал транзакций.</li><li>Диск занят базами, больше всего в <code>history_log</code>. Пытаемся почистить базу запросом, но она не уменьшается, потому что постгрес не отдаёт автоматически место ОС.</li><li>Disk I/O. Некто составил запрос на M таблиц в N потоков X тысяч раз в секунду. Результат — сервер пятисотит.</li><li>Тормоза фоновых процессов СУБД.</li><li>Накладные расходы от виртуализации</li><li>Хранилище от китайского noname</li><li>Дефолтные конфиги</li><li>&hellip;</li></ul><p>Что делать? Мониторить и планировать!</p><ul><li>Сразу на SSD, не раздумывая.</li><li>Схема данных: планировать заранее и писать только нужное. Например, не писать в логи JSON весом в несколько мегабайт.</li><li>Партиционирование</li></ul><p>Мониторинг:</p><ul><li>Storage: latency, utilization</li><li>PostgreSQL: подключенные клиенты, ошибки, запросы (statements)</li></ul><h2 id=масштабирование>Масштабирование</h2><p>Типичный разработчик видит БД как строчку конфига и не интересуется тем, как она работает. Это источник проблем.</p><p>OLTP-транзакции — быстрые, короткие, лёгкие.
OLAP-транзакции — медленные, долгие и тяжёлые.</p><p>Те и другие нужно разносить. Вторые мешают первым. Как масштабировать PostgreSQL так, чтобы разнести нагрузку?</p><ul><li>Streaming replication — создаём реплики, в том числе каскадные, на разные реплики вешаем разные запросы. Например, аналитику на отдельную реплику.</li><li>Logical publications, subscriptions. Конкретные наборы таблиц подключаем в соседние базы и их приложения могут ими пользоваться.</li><li>Внешние таблицы (foreigh tables), декларативное партиционирование (declarative partitioning). Несколько баз с разными наборами таблиц. С помощью механизма внешних таблиц объединяем их в одной базе, приложение работает с ней, но на самом деле качает данные из тех нескольких баз.</li></ul><p>С чего начать?</p><ul><li>С репликации. Разнести нагрузку на чтение и запись. Пишем в мастер, читаем из реплик. Для аналитических запросов делаем отдельную реплику.</li><li>Балансировка: лучше на стороне приложения, можно DNS round robin, интересно Keepalived и Haproxy, совсем хорошо Patroni, DCS.</li></ul><h2 id=приложения-и-субд-транзакции>Приложения и СУБД-транзакции</h2><p>Idle transactions приводят к снижению производительности, блокировкам и дедлокам, потом к пятисотым.</p><p>Откуда берутся такие транзакции?</p><ol><li><p>Внешний источник:</p><ol><li>Приложение открывает транзакцию</li><li>Потом идёт на какой-то другой источник, встречает там ошибку, падает.</li><li>Profit, транзакция висит, пока её явно не убьют.</li></ol></li><li><p>Нет обработки ошибок</p></li><li><p>Человеческий фактор — открыл и забыл.</p></li></ol><p>Что делать:</p><ul><li>Алерты в мониторинге</li><li><code>pg_terminate_backend</code></li><li>устранять ошибки в приложении</li></ul><p>Избегайте пустых транзакций любой ценой!</p><h2 id=велосипедостроение>Велосипедостроение</h2><p>Фоновая обработка событий. Бизнес хочет необычного. Появляются самописные очереди. От долгих транзакций эти таблицы распухают. Растёт время их обработки, очередь перестаёт работать.</p><p>Что делать: использовать Skytools PgQ. Но и в нём есть проблемы:</p><ul><li>Мало документации</li><li>Нужно применять джедайские техники</li><li>Много нужных знаний находятся в почтовых рассылках (mailing lists)</li></ul><p>Плюсы PgQ:</p><ul><li>Настроил и забыл.</li><li>Дешевле отдельных брокеров.</li></ul><p>Вывод: для каждой задачи сначала ищите инструменты, которые уже изобретены.</p><h2 id=автоматизация>Автоматизация</h2><p>Админы хотят:</p><ul><li>инстансы</li><li>деплой</li><li>конфиги</li></ul><p>Разработчики хотят:</p><ul><li>деплой</li><li>миграции катить без ручных вмешательств</li></ul><p>Все хотят autofailover! Но всё не так просто:</p><ul><li>Случается split-brain. Потеряли связность сети, fencing&rsquo;а нет, приложения пишут сразу в два мастера. Так было недавно у GitHub.</li><li>Каскадный failover. Мастер падает, переключаем на второй, который не успел отреплицироваться, он тоже падает, остаётся один сервер, который складывается под нагрузкой.</li></ul><p>Как делать failover?</p><ul><li>[-] bash-скрипты! (НЕТ!)</li><li>[-] ansible — баш на стероидах</li><li>[+] <a href=https://github.com/zalando/patroni>Patroni</a></li><li>[+] <a href=https://github.com/clusterlabs/PAF>PAF</a></li><li>[+] <a href=https://github.com/sorintlab/stolon>Stolon</a></li></ul><h2 id=контейнеры-и-оркестрация>Контейнеры и оркестрация</h2><p>А что, если развернуть базу в k8s?</p><ul><li>Нужно где-то хранить. Решения есть (CEPH, GlusterFS, DRBD), но они медленные. И нужно поддерживать кластерную файловую систему. Это работает, пока база маленькая, нет требований к производительности и не страшно потерять данные. Т.е. это не работает.</li></ul><p>Вывод: использовать для стейджингов и девелоперских машин, но не на проде. Если очень хочется — то использовать local volumes, streaming replication и операторы PostgreSQL.</p></div><div class=pagination><a href=/conf/aletheia/19/culture-transformation/ class="left arrow">&#8592;</a>
<a href=/conf/highload/18/1.2-per-aspera-ad-paas/ class="right arrow">&#8594;</a>
<a href=# class=top>Top</a></div></main><footer><span>&copy; <time datetime="2021-09-16 10:16:32.346781864 &#43;0000 UTC m=&#43;0.140891305">2021</time> . Made with <a href=https://gohugo.io>Hugo</a> using the <a href=https://github.com/EmielH/tale-hugo/>Tale</a> theme.</span></footer></body></html>